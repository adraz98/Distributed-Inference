# DistributedÂ AIÂ InferenceÂ System â€“ Coding Challenge Submission

> **Candidate:**Â *Adrian Azemi*\
> **Stack:** FastAPI Â· gRPC Â·Â asyncio Â· DockerÂ Compose Â· Streamlit

---

## Introduction

Build a **simulated, faultâ€‘tolerant distributed inference pipeline** on a single machine. The system:

- receives REST inference requests.
- microâ€‘batches and dispatch them to a fleet of worker processes via gRPC.
- survives worker crashes and network hiccups (retries (=redirects) & health checks).
- exposes logs, metrics and a live dashboard (containing also the logs).

---

## Highâ€‘LevelÂ Architecture

```text

                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                             â”‚        Dashboard (UI)        â”‚
                             â”‚          Streamlit           â”‚
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                           REST (polls `/status`, `/recent_requests`)
                                            â”‚
                                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      HTTP/REST      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      gRPC        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Test Script     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚            Coordinator (FastAPI + asyncio)    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    Worker 1      â”‚
â”‚ (bursty load)    â”‚                     â”‚  â€¢ Micro-batch queue                          â”‚      gRPC        â”‚  (DistilBERT)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚  â€¢ Round-robin load-balancer                  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    Worker 2      â”‚
                                         â”‚  â€¢ Retry logic & heart-beats                  â”‚      gRPC        â”‚  (DistilBERT)    â”‚
                                         â”‚  â€¢ Structured JSON logging                    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    Worker 3      â”‚
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚  (DistilBERT)    â”‚
                                                                                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

- **Coordinator** â€“ Stateless FastAPI app. Queues requests, builds microâ€‘batches (size/time), loadâ€‘balances across healthy workers, handles retries (=redirections to other healthy workers), and conducts logging.
- **Workers** â€“ Lightweight Python processes running DistilBERTâ€‘SST2 in **ONNXRuntimeâ€‘CPU**Â classifying a text sequence into **positive** or **negative**. Each worker exposes a gRPC service generated from `proto/inference.proto`.
- **ClientÂ &Â LoadÂ Test** â€“ A small Python test script firing bursty traffic to validate throughput and faultâ€‘tolerance.
- **Dashboard** â€“ Streamlit frontâ€‘end pulling `/status` & `/recent_requests`Â from the coordinator for nearâ€‘realâ€‘time monitoring.

---

## WhyÂ gRPCÂ Internally?

| Feature                                            | gRPCÂ (HTTP/2)      | ClassicÂ REST (HTTP/1.1)   |
| -------------------------------------------------- | ------------------ | ------------------------- |
| **Multiplexing** (many concurrent streams per TCP) | âœ”                  | âŒ (headâ€‘ofâ€‘line blocking) |
| **Binary Framing** (ProtoBuf)                      | âœ” (compact & fast) | âŒ (text JSON)             |
| **Biâ€‘directional Streaming**                       | âœ”                  | âŒ                         |
| **Code Generation** (typed stubs)                  | âœ”                  | âŒ                         |
| **Lowâ€‘latencyÂ &Â CPU**                              | âœ”                  | âŒ                         |

REST remains for external clients conducting inference requests because it is humanâ€‘friendly and trivial to test/observe with `curl` or a browser.

---

## RepositoryÂ Layout

```text
â”œâ”€â”€ coordinator/          # FastAPI service (app.py) + gRPC proto
â”‚   â”œâ”€â”€ proto/            
â”‚   â”œâ”€â”€ app.py            # Coordinator logic
â”‚   â””â”€â”€ Dockerfile        # Coordinator Docker image
â”‚
â”œâ”€â”€ worker/               # gRPC worker service
â”‚   â”œâ”€â”€ models/           # distilbert-sst2.onnx model
â”‚   â”œâ”€â”€ proto/            
â”‚   â”œâ”€â”€ worker.py         # Worker logic
â”‚   â””â”€â”€ Dockerfile        # Worker Docker image
â”‚
â”œâ”€â”€ monitor/              # Streamlit dashboard UI
â”‚   â”œâ”€â”€ dashboard.py      # Streamlit app
â”‚   â””â”€â”€ Dockerfile        # Dashboard Docker image
â”‚
â”œâ”€â”€ proto/                # Shared proto definitions
â”‚   â””â”€â”€ inference.proto   # gRPC service definitions
â”‚
â”œâ”€â”€ test/                 # Test client
â”‚   â”œâ”€â”€ test_client.py    # Bursty load test script
â”‚   â””â”€â”€ Dockerfile        # Test client Docker image
â”‚
â”œâ”€â”€ docs/                 # Documentation assets
â”‚   â””â”€â”€ images/           # Example dashboard screenshots
â”‚
â”œâ”€â”€ docker-compose.yml    # Orchestration for coordinator, workers, dashboard, test client
â””â”€â”€ README.md             # (this file)
```

---

## QuickÂ Start & Testing (ğŸ› Â DockerÂ Compose)

> **Prerequisites:** DockerÂ â‰¥â€¯28, DockerÂ ComposeÂ v2.

```bash
# 1ï¸âƒ£Â Clone
$ git clone https://github.com/yourname/distributedâ€‘aiâ€‘challenge.git
$ cd distributedâ€‘aiâ€‘challenge

# 2ï¸âƒ£Â Build & Run (coordinator + 3 workers + dashboard + test client)
$ docker compose up --build

# 4ï¸âƒ£Â Open dashboard Streamlit UI)
http://localhost:8501  # (containing ALL live and historical logged information)
```

The compose file ensures that all services are started automatically in the correct order. **After running the above commands,
you can observe the results of the test script in the Streamlit dashboard.**

### Expected and exemplary outputs

When running Docker Compose with the default parameters, you will simulate ideal network conditions. In this case, the 
output should resemble the following:

![expected with ideal network.png](docs/images/expected%20with%20ideal%20network.png)

If you adjust the parameters to simulate network failures and delays, the output may vary significantly. This is 
particularly noticeable if all three workers fail simultaneously for a short period. Although Docker automatically 
restarts failed workers, many requests may still be lost during such events. The same can hold when by chance unusually 
many simulated network delays trigger the worker timeouts. Below is an example of potential output under simulated network
failures and delays (with the commented out parameters in `docker-compose.yml`):

![example simulated network failure.png](docs/images/example%20simulated%20network%20failure.png)

---

## Configuration

All knobs are environment variables â€‘â€‘ override them in `dockerâ€‘compose.yml`.

| Variable               | Default                                     | Scope       | Description                                     |
| ---------------------- |---------------------------------------------| ----------- | ----------------------------------------------- |
| **WORKER\_TIMEOUT**    | `3`                                         | coordinator | Seconds to wait for a single gRPC call          |
| **BATCH\_SIZE**        | `3`                                         | coordinator | Maximum microâ€‘batch size                        |
| **BATCH\_TIMEOUT**     | `0.15`                                      | coordinator | Max age (s) of oldest queued task before dispatch |
| **WORKERS**            | `worker1:50051,worker2:50051,worker3:50051` | coordinator | Commaâ€‘separated gRPC addresses                  |
| **REQUEST\_LOG\_SIZE** | `5000`                                      | coordinator | Ring buffer length for `/recent_requests`       |
| **CRASH\_RATE**        | `0` respectively `0.05` (for every worker)  | worker      | ProbabilityÂ âˆˆÂ [0,1] that a worker crashes  |
| **MAX\_DELAY**         | `0` respectively `4` (for every worker)     | worker      | Synthetic network/compute latencyÂ [s] upperâ€‘bound |


---

## RESTÂ APIÂ Reference

### `POST /infer`

| Purpose      | Submit a single inference request                                                                 |
| ------------ | ------------------------------------------------------------------------------------------------- |
| RequestÂ Body | `{ "input": "<text>" }`                                                                           |
| SuccessÂ 200  | `{ "request_id": "<uuid>", "output": "<text>", "worker": "<id>", "success": true, "retries": 0 }` |
| FailureÂ 5xx  | `{ "detail": "Inference request timed out" }`                                                     |

> The call blocks until the task succeeds **or** â‰ˆâ€¯`5Â Ã—Â WORKER_TIMEOUT` (â‰ˆâ€¯15â€¯s default).

---

### `GET /status`

Returns coordinator health snapshot, worker states and aggregated counters. Example:

```json
{
  "workers": [
    { "id": "worker1", "alive": true,  "tasks_processed": 1287, "last_seen": 1721128100.23 },
    { "id": "worker2", "alive": true,  "tasks_processed": 1293, "last_seen": 1721128100.18 },
    { "id": "worker3", "alive": true,  "tasks_processed": 1286, "last_seen": 1721128099.95 }
  ],
  "tasks_submitted": 3900,
  "tasks_completed": 3886,
  "tasks_failed": 14,
  "tasks_retried": 71,
  "queue_length": 0
}
```

---

## FailureÂ &Â LatencyÂ Simulation

Uncomment or tweak the following in `dockerâ€‘compose.yml`:

```yaml
environment:
  CRASH_RATE: "0.05"   # 5Â % chance the worker exits midâ€‘batch
  MAX_DELAY:  "2"      # Each request delayed âˆˆÂ [0Â ..Â 2]Â s
```

*Docker will autoâ€‘restart crashed containers*. You can monitor the impact live on the Streamlit dashboard â€“ look for âŒ workers, retries and failed requests and observe how Docker brings them back up.

---

## MonitoringÂ Dashboard

```bash
http://localhost:8501
```

The UI shows:

- **System Health** â€“ âœ…/âŒ per worker + last heartbeat
- **CoreÂ Metrics** â€“ submitted/completed/failed/retried counters, current queue length
- **PendingÂ Requests** â€“ â³ queued & âš™ï¸ inâ€‘flight requests currently being processed by workers (live table)
- **HistoricalÂ Logs** â€“ 5â€¯000 mostâ€‘recent requests incl. latency, retries, assigned workers, errors, truncatedÂ input & model output

An illustration of the dashboard depicting all these aspects at once is shown below:

![example simulated network failure pending.png](docs/images/example%20simulated%20network%20failure%20pending.png)

---

*Thanks for reviewing my submission!*

yolo3